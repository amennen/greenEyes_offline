{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/beta/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/jukebox/pkgs/PYGER/beta/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "from nilearn.image import resample_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "from nilearn.plotting import show\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn import image\n",
    "from nilearn.masking import apply_mask\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image\n",
    "from nilearn.input_data import NiftiMasker\n",
    "#from nilearn import plotting\n",
    "import nibabel\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn.image import load_img\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn.input_data import NiftiMasker,  MultiNiftiMasker\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import LeaveOneLabelOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFwe\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "from scipy import interp\n",
    "\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (5, 3),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize': 'x-large',\n",
    "          'xtick.labelsize': 'x-large',\n",
    "          'ytick.labelsize': 'x-large'}\n",
    "font = {'weight': 'bold',\n",
    "        'size': 22}\n",
    "plt.rc('font', **font)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, GenericUnivariateSelect, SelectKBest, chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scipy import stats\n",
    "import brainiak\n",
    "import brainiak.funcalign.srm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectDir='/jukebox/norman/amennen/prettymouth_fmriprep2/'\n",
    "DMNmask='/jukebox/norman/amennen/MNI_things/Yeo_JNeurophysiol11_MNI152/Yeo_Network7mask_reoriented_resampledBOLD2.nii.gz'\n",
    "fmriprep_dir=projectDir + '/derivatives/fmriprep'\n",
    "preproc_dir = projectDir + '/derivatives/preproc'\n",
    "TOM_large=projectDir + 'ROI/TOM_large_resampled_maskedbybrain.nii.gz'\n",
    "TOM_cluster=projectDir + 'ROI/TOM_cluster_resampled_maskedbybrain.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject numbers\n",
    "subInd = 0\n",
    "nsub=38\n",
    "allnames = []\n",
    "allgroups = []\n",
    "groupInfo={}\n",
    "# skip subjects 039 and 116\n",
    "with open(projectDir + 'participants.tsv') as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        if 'sub' in row[0]:\n",
    "            # now skip the subjects we don't want to analyze\n",
    "            allInfo = row[0].split('\\t')\n",
    "            subjName=allInfo[0]\n",
    "            if subjName != 'sub-039' and subjName != 'sub-116':\n",
    "                if allInfo[3] == 'paranoia':\n",
    "                    group = 0\n",
    "                elif allInfo[3] == 'affair':\n",
    "                    group = 1\n",
    "                allnames.append(subjName)\n",
    "                allgroups.append(group)\n",
    "                subInd+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-088', 'sub-089', 'sub-090', 'sub-091', 'sub-092', 'sub-093', 'sub-094', 'sub-095', 'sub-096', 'sub-097', 'sub-098', 'sub-099', 'sub-100', 'sub-101', 'sub-102', 'sub-103', 'sub-104', 'sub-105', 'sub-106', 'sub-107', 'sub-108', 'sub-109', 'sub-110', 'sub-111', 'sub-068', 'sub-081', 'sub-112', 'sub-053', 'sub-113', 'sub-031', 'sub-114', 'sub-115', 'sub-117', 'sub-118', 'sub-119', 'sub-120', 'sub-121', 'sub-122']\n"
     ]
    }
   ],
   "source": [
    "paranoidSubj = allnames[0:19]\n",
    "cheatingSubj = allnames[19:]\n",
    "paranoidLabel = allgroups[0:19]\n",
    "cheatingLabel = allgroups[19:]\n",
    "nfolds=19\n",
    "print(allnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "sub-088\n",
      "1\n",
      "sub-089\n",
      "2\n",
      "sub-090\n",
      "3\n",
      "sub-091\n",
      "4\n",
      "sub-092\n",
      "5\n",
      "sub-093\n",
      "6\n",
      "sub-094\n",
      "7\n",
      "sub-095\n",
      "8\n",
      "sub-096\n",
      "9\n",
      "sub-097\n",
      "10\n",
      "sub-098\n",
      "11\n",
      "sub-099\n",
      "12\n",
      "sub-100\n",
      "13\n",
      "sub-101\n",
      "14\n",
      "sub-102\n",
      "15\n",
      "sub-103\n",
      "16\n",
      "sub-104\n",
      "17\n",
      "sub-105\n",
      "18\n",
      "sub-106\n",
      "19\n",
      "sub-107\n",
      "20\n",
      "sub-108\n",
      "21\n",
      "sub-109\n",
      "22\n",
      "sub-110\n",
      "23\n",
      "sub-111\n",
      "24\n",
      "sub-068\n",
      "25\n",
      "sub-081\n",
      "26\n",
      "sub-112\n",
      "27\n",
      "sub-053\n",
      "28\n",
      "sub-113\n",
      "29\n",
      "sub-031\n",
      "30\n",
      "sub-114\n",
      "31\n",
      "sub-115\n",
      "32\n",
      "sub-117\n",
      "33\n",
      "sub-118\n",
      "34\n",
      "sub-119\n",
      "35\n",
      "sub-120\n",
      "36\n",
      "sub-121\n",
      "37\n",
      "sub-122\n"
     ]
    }
   ],
   "source": [
    "# first load in data, remove first 2 TRs, resave\n",
    "n_trunc=2 # Number of volumes to trim/truncate\n",
    "\n",
    "nSub = 38\n",
    "for s in np.arange(nSub):\n",
    "    print(s)\n",
    "    subjName=allnames[s]\n",
    "    print(subjName)\n",
    "    subjData=fmriprep_dir + '/' + subjName + '/' + 'func' + '/' + subjName + '_task-prettymouth_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "    epi_data = nib.load(subjData)\n",
    "    epi=epi_data.get_fdata()\n",
    "    epi_trunc =np.zeros((epi_data.shape[0], epi_data.shape[1], epi_data.shape[2], epi_data.shape[3]-n_trunc))\n",
    "    epi_trunc[:, :, :, :] = epi[:,:,:,n_trunc:]\n",
    "    #print(epi_data.shape, '  ', epi_trunc.shape)\n",
    "    dimsize=epi_data.header.get_zooms()\n",
    "    #print(dimsize)\n",
    "    affine_mat = epi_data.affine  # What is the orientation of the data\n",
    "    #print(affine_mat)\n",
    "    # now save\n",
    "    output_name = preproc_dir + '/' + subjName + '/' + 'func' + '/' + subjName + '_task-prettymouth_space-MNI152NLin2009cAsym_desc-preproc_bold_truncTRs.nii.gz'\n",
    "    bold_nii = nib.Nifti1Image(epi_trunc, affine_mat)\n",
    "    hdr = bold_nii.header  # get a handle for the .nii file's header\n",
    "    hdr.set_zooms((dimsize[0], dimsize[1], dimsize[2], epi_trunc.shape[3]))\n",
    "    if not os.path.exists(os.path.dirname(output_name)):\n",
    "        os.makedirs(os.path.dirname(output_name))\n",
    "    nib.save(bold_nii, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-088\n",
      "sub-089\n",
      "sub-090\n",
      "sub-091\n",
      "sub-092\n",
      "sub-093\n",
      "sub-094\n",
      "sub-095\n",
      "sub-096\n",
      "sub-097\n",
      "sub-098\n",
      "sub-099\n",
      "sub-100\n",
      "sub-101\n",
      "sub-102\n",
      "sub-103\n",
      "sub-104\n",
      "sub-105\n",
      "sub-106\n",
      "sub-107\n",
      "sub-108\n",
      "sub-109\n",
      "sub-110\n",
      "sub-111\n",
      "sub-068\n",
      "sub-081\n",
      "sub-112\n",
      "sub-053\n",
      "sub-113\n",
      "sub-031\n",
      "sub-114\n",
      "sub-115\n",
      "sub-117\n",
      "sub-118\n",
      "sub-119\n",
      "sub-120\n",
      "sub-121\n",
      "sub-122\n"
     ]
    }
   ],
   "source": [
    "# now load in the new truncated and highpass filter with two different versions\n",
    "n_trunc=2 # Number of volumes to trim/truncate\n",
    "nSub = 38\n",
    "\n",
    "filterY = 337.75 # Yaara's filter\n",
    "filterS = 140 # Sam's filter\n",
    "# make masker\n",
    "##### FOR DMN\n",
    "#nVox=3757\n",
    "#### FOR TOM_Large\n",
    "#nVox=2414\n",
    "#### FOR CLUSTERS\n",
    "nVox = 240\n",
    "nTR = 475 - n_trunc\n",
    "aggregate_masked_trunc_filt_Y = np.zeros((nVox,nTR,nSub))\n",
    "aggregate_masked_trunc_filt_S = np.zeros((nVox,nTR,nSub))\n",
    "for s in np.arange(nSub):\n",
    "    subjName=allnames[s]\n",
    "    print(subjName)\n",
    "    epi_file = preproc_dir + '/' + subjName + '/' + 'func' + '/' + subjName + '_task-prettymouth_space-MNI152NLin2009cAsym_desc-preproc_bold_truncTRs.nii.gz'\n",
    "\n",
    "    epi_masker = NiftiMasker(mask_img = TOM_cluster,\n",
    "                             high_pass=1/filterY,\n",
    "                             standardize=False,\n",
    "                             t_r=1.5,\n",
    "                             memory='nilearn_cache',\n",
    "                             memory_level=1,\n",
    "                             verbose=0\n",
    "                            )\n",
    "    epi_mask_dataY = epi_masker.fit_transform(epi_file)\n",
    "    aggregate_masked_trunc_filt_Y[:,:,s] = epi_mask_dataY.T\n",
    "\n",
    "    epi_masker = NiftiMasker(mask_img = TOM_cluster,\n",
    "                             high_pass=1/filterS,\n",
    "                             standardize=False,\n",
    "                             t_r=1.5,\n",
    "                             memory='nilearn_cache',\n",
    "                             memory_level=1,\n",
    "                             verbose=0\n",
    "                            )\n",
    "    epi_mask_dataS = epi_masker.fit_transform(epi_file)\n",
    "    aggregate_masked_trunc_filt_S[:,:,s] = epi_mask_dataS.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the two different aggregate data types\n",
    "#np.save('aggregate_data_trunc_filtY', aggregate_masked_trunc_filt_Y)\n",
    "#np.save('aggregate_data_trunc_filtS', aggregate_masked_trunc_filt_S)\n",
    "#np.save('aggregate_data_TOM_large_trunc_filtY', aggregate_masked_trunc_filt_Y)\n",
    "#np.save('aggregate_data_TOM_large_trunc_filtS', aggregate_masked_trunc_filt_S)\n",
    "np.save('aggregate_data_TOM_cluster_trunc_filtY', aggregate_masked_trunc_filt_Y)\n",
    "np.save('aggregate_data_TOM_cluster_trunc_filtS', aggregate_masked_trunc_filt_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-088\n",
      "sub-089\n",
      "sub-090\n",
      "sub-091\n",
      "sub-092\n",
      "sub-093\n",
      "sub-094\n",
      "sub-095\n",
      "sub-096\n",
      "sub-097\n",
      "sub-098\n",
      "sub-099\n",
      "sub-100\n",
      "sub-101\n",
      "sub-102\n",
      "sub-103\n",
      "sub-104\n",
      "sub-105\n",
      "sub-106\n",
      "sub-107\n",
      "sub-108\n",
      "sub-109\n",
      "sub-110\n",
      "sub-111\n",
      "sub-068\n",
      "sub-081\n",
      "sub-112\n",
      "sub-053\n",
      "sub-113\n",
      "sub-031\n",
      "sub-114\n",
      "sub-115\n",
      "sub-117\n",
      "sub-118\n",
      "sub-119\n",
      "sub-120\n",
      "sub-121\n",
      "sub-122\n"
     ]
    }
   ],
   "source": [
    "# DO THIS FOR THE DATA THAT ISN'T TRUNCATED/FILTERED****\n",
    "#nVox = 3757 DMN mask\n",
    "#nVox = 2414 # TOM_large\n",
    "nVox = 240 # TOM clusster\n",
    "nTRs = 475\n",
    "nSub = 38\n",
    "aggregate_masked_data = np.zeros((nVox,nTRs,nSub))\n",
    "for s in np.arange(nsub):\n",
    "    subjName=allnames[s]\n",
    "    print(subjName)\n",
    "    subjData=fmriprep_dir + '/' + subjName + '/' + 'func' + '/' + subjName + '_task-prettymouth_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "    #masked_data = apply_mask(subjData, DMNmask)\n",
    "    masked_data = apply_mask(subjData,TOM_cluster) # CHANGE HERE TOO\n",
    "    aggregate_masked_data[:,:,s] = masked_data.T\n",
    "\n",
    "#np.save('aggregate_data.npy', aggregate_masked_data)\n",
    "np.save('aggregate_data_TOM_cluster.npy', aggregate_masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aggregate_data_TOM_cluster.npy', aggregate_masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
